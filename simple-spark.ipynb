{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark with Jupyter\n",
    "\n",
    "## Import the module\n",
    "\n",
    "This is already installed in the docker container, so simply import it here.\n",
    "\n",
    "## Create a Spark Context\n",
    "\n",
    "This should only be done once per session.  Guarding the creation with the \"try\" block ensures that we will only create the context the first time the cell is executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "\n",
    "# Create a Spark context for local work\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    sc = pyspark.SparkContext('local[*]')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prove the module is available\n",
    "\n",
    "Create a simple example and execute it in order to demonstrate that the module working correctly and the context is configured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[391, 630, 325, 456, 673]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prove that Spark is installed and working correctly\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Forest CoverType dataset\n",
    "\n",
    "The _Covtype_ data set is available online from: https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/\n",
    "The file _covtype.data.gz_ includes the type data and _covtype.info_ includes the metadata.  This is makde available from UC Irvine Machine Learning Repository and was originally provided by Colorado State University.\n",
    "\n",
    "This dataset has also been used in a *Kaggle* competition (https://www.kaggle.com/c/forest-cover-type-prediction).\n",
    "\n",
    "First lets make sure that the file is available locally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "localfile = 'data/covtype.data.gz'\n",
    "localfile, headers = urllib.request.urlretrieve(url, localfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data locally, we can grab it through an RDD.  Use a simple textfile RDD since this is an ASCII CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData = sc.textFile(localfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform the raw data into a sequence of LabeledPoints (from MLLib):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Extract the dataset features and target.\n",
    "def ingest(line):\n",
    "    # Simple numeric features (some are one-hot encoded).\n",
    "    # Last field is the label (training target).\n",
    "    fields = [float(f) for f in line.split(',')]\n",
    "    features = Vectors.dense(fields[0:len(fields)-1])\n",
    "    \n",
    "    # Subtract 1 from the label to satisfy the '0' based\n",
    "    # DecisionTree model.\n",
    "    label    = fields[-1] - 1\n",
    "    return LabeledPoint(label,features)\n",
    "\n",
    "pointdata = rawData.map(ingest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data in a format we can train a model with, lets look at a few entries to see if it is as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(4.0, [2596.0,51.0,3.0,258.0,0.0,510.0,221.0,232.0,148.0,6279.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(4.0, [2590.0,56.0,2.0,212.0,-6.0,390.0,220.0,235.0,151.0,6225.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [2804.0,139.0,9.0,268.0,65.0,3180.0,234.0,238.0,135.0,6121.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [2785.0,155.0,18.0,242.0,118.0,3090.0,238.0,238.0,122.0,6211.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(4.0, [2595.0,45.0,2.0,153.0,-1.0,391.0,220.0,234.0,150.0,6172.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointdata.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we see that we have a sequence of LabeledPoint objects, each with a label (numeric in this case, since we ensured that the values would be converted to float values).  There is a DenseMatrix element included for each point, with 55 (float) values corresponding to the features.\n",
    "\n",
    "The first 10 features are numeric.  The next two are categorical and have been encoded as one-hot codes.  The _Wilderness Areas_ feature takes 4 columns, and the _Soil Type_ takes up 40 columns.\n",
    "\n",
    "The _Cover Type_ factor is used as the label is a numerically encoded categorical factor.  It is important to ensure that this encoded value is not treated as a number - no ordering in the feature is implied.  Each value stands alone and simply indicates a category and not a relationship with any other feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Partitioning\n",
    "\n",
    "At this point, we can partition the data into a training set, a validation set, and a testing set.  We will use the training set to train the model, and the validation set to measure how our training is performing.  We can adjust model hyperparameters to adjust training results as measured by the validation dataset without issue.\n",
    "\n",
    "We will only run predictions against the testing dataset and no information from testing will be used during training or validation.  This keeps our confidence in the test performance measurements high.\n",
    "\n",
    "For this example we can simply split the data randomly into the 3 sets of data, with 80% used for training, and 10% used for each of validation and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "464813 58010 58189\n"
     ]
    }
   ],
   "source": [
    "trainData, cvData, testData = pointdata.randomSplit([0.8,0.1,0.1])\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "testData.cache()\n",
    "\n",
    "print(trainData.count(),cvData.count(),testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the data partitioning has separated the original dataset into suitable size segments that we can use for our different purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Decision Tree Model\n",
    "\n",
    "At this point we can go ahead and train the model.  Here we are using the Spark MLLib DecisionTree (http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.tree.DecisionTree) and DecisionTreeModel (http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.tree.DecisionTreeModel) for this purpose.  Read more about these and the hyperparameters available from the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "from pyspark.mllib.util import MLUtils\n",
    "\n",
    "model = DecisionTree.trainClassifier( trainData, 7, {}, \"gini\", 4, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have a trained model and we would like to evaluate its performance.  We do this with the validation data.  If we find that we want to increase the performance, we can adjust hyperparameters or even select a different model type depending on the outcome from this evaluation.\n",
    "\n",
    "Using the MLLib MulticlassMetrics object allows us to very simply evaluate the prediction performance of the model.  Here we perform the predictions, then form a labelled set of results that can be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "cvPredictions = model.predict( cvData.map(lambda x: x.features))\n",
    "cvLabelsAndPredictions = cvData.map(lambda x: x.label).zip(cvPredictions)\n",
    "\n",
    "cvMetrics = MulticlassMetrics(cvLabelsAndPredictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can extract some performance results from the metrics object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision:  0.7023789001896225 \n",
      "                    recall:  0.7023789001896225 \n",
      "                  fMeasure:  0.7023789001896225 \n",
      "         weightedPrecision:  0.743841763253158 \n",
      "            weightedRecall:  0.7023789001896226 \n",
      " weightedFalsePositiveRate:  0.18384149482164294 \n",
      "  weightedTruePositiveRate:  0.7023789001896226 \n",
      " \n",
      "Confusion Matrix:\n",
      "[[14341  5586     0     0     0  1153]\n",
      " [ 6438 22322   433     0   516    30]\n",
      " [   11   356  3010   159  1144     0]\n",
      " [    0    17    61   126    31     0]\n",
      " [    1     9    12     0    56     0]\n",
      " [  328    34     0     0     0   890]]\n"
     ]
    }
   ],
   "source": [
    "print('                 precision: ',cvMetrics.precision(),'\\n',\n",
    "      '                   recall: ',cvMetrics.recall(),'\\n',\n",
    "      '                 fMeasure: ',cvMetrics.fMeasure(),'\\n',\n",
    "      '        weightedPrecision: ',cvMetrics.weightedPrecision,'\\n',\n",
    "      '           weightedRecall: ',cvMetrics.weightedRecall,'\\n',\n",
    "      'weightedFalsePositiveRate: ',cvMetrics.weightedFalsePositiveRate,'\\n',\n",
    "      ' weightedTruePositiveRate: ',cvMetrics.weightedTruePositiveRate,'\\n',\n",
    "      '\\nConfusion Matrix:'\n",
    "     )\n",
    "\n",
    "import numpy as np\n",
    "print(cvMetrics.confusionMatrix().toArray().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we are satisfied with the models predictive performance (hint - the performance above is not very satisfying!), we can then evaluate the model on the test data, which we expect will give slightly lower performance than the validation data since some work flows include iteration over the validation data more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testPredictions = model.predict( testData.map(lambda x: x.features))\n",
    "testLabelsAndPredictions = testData.map(lambda x: x.label).zip(testPredictions)\n",
    "\n",
    "testMetrics = MulticlassMetrics(testLabelsAndPredictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision:  0.7043083744350307 \n",
      "                    recall:  0.7043083744350307 \n",
      "                  fMeasure:  0.7043083744350307 \n",
      "         weightedPrecision:  0.7452877071581182 \n",
      "            weightedRecall:  0.7043083744350307 \n",
      " weightedFalsePositiveRate:  0.18281720128587403 \n",
      "  weightedTruePositiveRate:  0.7043083744350307 \n",
      " \n",
      "Confusion Matrix:\n",
      "[[14309  5654     0     0     0  1121]\n",
      " [ 6285 22506   468     0   529    43]\n",
      " [   10   338  3142   150  1127     0]\n",
      " [    0    18    65   106    28     0]\n",
      " [    0     4    10     0    61     0]\n",
      " [  343    37     0     0     0   859]]\n"
     ]
    }
   ],
   "source": [
    "print('                 precision: ',testMetrics.precision(),'\\n',\n",
    "      '                   recall: ',testMetrics.recall(),'\\n',\n",
    "      '                 fMeasure: ',testMetrics.fMeasure(),'\\n',\n",
    "      '        weightedPrecision: ',testMetrics.weightedPrecision,'\\n',\n",
    "      '           weightedRecall: ',testMetrics.weightedRecall,'\\n',\n",
    "      'weightedFalsePositiveRate: ',testMetrics.weightedFalsePositiveRate,'\\n',\n",
    "      ' weightedTruePositiveRate: ',testMetrics.weightedTruePositiveRate,'\\n',\n",
    "      '\\nConfusion Matrix:'\n",
    "     )\n",
    "\n",
    "print(testMetrics.confusionMatrix().toArray().astype(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
