{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PySpark with Jupyter\n",
    "\n",
    "PySpark is an interface into the Apache Spark framework:\n",
    "\n",
    "> Apache Spark is an open-source cluster computing framework originally developed in the AMPLab at UC Berkeley. In contrast to Hadoop's two-stage disk-based MapReduce paradigm, Spark's in-memory primitives provide performance up to 100 times faster for certain applications.\n",
    "\n",
    "Spark is used for big data applications since, by definition, they are not able to be processed within a single compute resource.  A common use for the framework is to process large amounts of data and use Machine Learning techniques to analyze, understand, and predict outcomes for external processes.\n",
    "\n",
    "This notebook was created by aggregating information from various sources, including notebooks and code that I have developed on projects, but also using some of the following books:, [Learning Spark](http://shop.oreilly.com/product/0636920028512.do), [Advanced Analytics with Spark](http://shop.oreilly.com/product/0636920035091.do), and [High Performance Spark](http://shop.oreilly.com/product/0636920046967.do)\n",
    "\n",
    "Some of these resources do not include Python or PySpark usage directly, but I have been able to translate the information into Pythonic, or at least Python, for use here.\n",
    "\n",
    "In addition, many resources exist on the web for exploring [Python](https://www.python.org/) and [PySpark](http://spark.apache.org/docs/latest/api/python/index.html) as well as Machine Learning and other big data uses in general.  Due to the dynamic nature of these resources, you should always search and use the most current information available at the time you need it.\n",
    "\n",
    "## Import the module\n",
    "\n",
    "This is already installed in the docker container, so simply import it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Spark Context\n",
    "\n",
    "Creating a SparkContext requires the configuration for Spark operation to be defined.  This is most easily done by creating a SparkConf object with the desired parameter values for the way you want Spark to operate.  Here we define a 'local' style operation since we want to explore Spark and PySpark without needing to have a cluster available for job execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "[('spark.master', 'local[*]'),\n",
      " ('spark.submit.deployMode', 'client'),\n",
      " ('spark.app.name', 'Introduction Notebook')]\n"
     ]
    }
   ],
   "source": [
    "# Create a simple local Spark configuration.\n",
    "conf = (\n",
    "    pyspark\n",
    "      .SparkConf()\n",
    "      .setMaster('local[*]')\n",
    "      .setAppName('Introduction Notebook')\n",
    ")\n",
    "\n",
    "# Show the configuration:\n",
    "import pprint as pp\n",
    "print('Configuration:')\n",
    "pp.pprint(conf.getAll())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Creating a context should only be done once per session.  Guarding the creation with the \"try\" block ensures that we will only create the context the first time the following cell is executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version:  1.6.1\n"
     ]
    }
   ],
   "source": [
    "# Create a Spark context for local work.\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    sc = pyspark.SparkContext(conf = conf)\n",
    "\n",
    "# Check that we are using the expected version of PySpark.\n",
    "print('Version: ',sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prove the module is available\n",
    "\n",
    "Create a simple example and execute it in order to demonstrate that the module working correctly and the context is configured correctly.\n",
    "\n",
    "The following creates an RDD initialized with a range of numbers, then samples 5 of them.  Spark will have distributed the RDD data and the work execution among the available executors in order to perform this processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[390, 464, 204, 403, 203]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prove that Spark is installed and working correctly\n",
    "rdd = sc.parallelize(range(1000))\n",
    "rdd.takeSample(False, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Some Text\n",
    "\n",
    "Once we have a working Spark instance, we can perform some actual work.  A common example is to perform word counting on a corpus of text.\n",
    "\n",
    "For the following, the full text for Shakespear's _The Taming of the Shrew_ was obtained and will be processed.  The text was obtained from [lexically.net](http://lexically.net/wordsmith/support/shakespeare.html) which obtained the actual corpus from the [Online Library of Liberty](http://www.lexically.net).\n",
    "\n",
    "If you download and process these files yourself, note that they are stored in 16 bit Unicode.  For simplicity, there is a local copy of this one play located in the _data_ directory that is stored in UTF8 format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# UTF8 encoded textfile.\n",
    "shrewText = sc.textFile('data/tamingoftheshrew.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most text processing and NLP work flows, removal of stop words reduces the size of the required tasks.  We can use the standard stop words available from the _stop-words_  Python package for our list of words to remove from the corpus.  Since we are processing the text in Spark, we go ahead and _broadcast_ this data to all workers.  This is an efficient mechanism to ensure that all workers have the data available with a minimum of network traffic involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Grab stop words to remove from the corpus.\n",
    "from stop_words import get_stop_words\n",
    "stopwords = sc.broadcast( set(get_stop_words('en')) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the corpus to process and some immutable data to work with, we can start working on the data.  First, we split the input into individual words.  We can easily do this by splitting on whitespace of any kind and size, then creating an output record for each word resulting from the split.\n",
    "\n",
    "In the code below, the splitting is done internal to the flatMap call.  In that call, each line is processed to replace whitespace of any kind with a single space, all text is converted to lower case for counting, and then the single spaces are used to split the line into a record for each word.\n",
    "\n",
    "Note that the multiple whitespace portions of a line are found and replaced using a regular expression pattern.  This pattern was broadcast to all worker processes since the pattern itself is immutable and common to all workers.  Note that use of the broadcast variables requires that the _.value_ attribute be accessed to obtain the original variable.\n",
    "\n",
    "Once the words have been converted to records, the stop words are removed and any remaining empty records are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate rows by splitting at (any number of) spaces.\n",
    "import re\n",
    "pattern = sc.broadcast( re.compile(r'\\s+') )\n",
    "shrewWords  = (\n",
    "    shrewText.flatMap(lambda line: pattern.value.sub(' ',line.strip().lower()).split(\" \"))\n",
    "      .filter(lambda w: w not in stopwords.value) # Remove stop words.\n",
    "      .filter(lambda w: len(w) > 0)               # Remove empty words.\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a record for each individual word in the corpus, we can group and count them.  To do this we create Key/Value records by mapping the words.  The Key is set to the word, and the Value is given an integer value of 1.\n",
    "\n",
    "Calling _.recduceByKey()_ on these Key/Value records groups the records for each work together and processes then using the provided function.  Here we add up all of the individual Values for the records of that Key.  Since each word in the corpus started with a Value of 1, adding these together results in the count of the number of times that (Key) word appears in the corpus.\n",
    "\n",
    "We go on to sort the result in descending order by count, then save the result.  The saved result is stored in parts by the RDD and will need to be combined in order to see the entire output together.  Other storage types can provide a single output file for review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Count the words by mapping a value for each row\n",
    "# and adding the values up for each unique key.\n",
    "shrewCounts = (\n",
    "    shrewWords.map(lambda word: (word, 1)) # Generate the Key/Value records.\n",
    "      .reduceByKey(lambda x, y: x + y)     # Generate the word counts.\n",
    "      .map(lambda t: (t[1],t[0]))          # Swap Key and Value to sort by Value.\n",
    "      .sortByKey(False)                    # Sort in descending order.\n",
    "      .map(lambda t: (t[1],t[0]))          # Swap back to original sense of Key/Value.\n",
    ")\n",
    "\n",
    "resultsLocation = 'shrewcounts'\n",
    "\n",
    "# Ensure that there is no previous output in the location.\n",
    "# Choose to store multiple results by using multiple locations.\n",
    "import shutil\n",
    "shutil.rmtree(resultsLocation,ignore_errors=True)\n",
    "\n",
    "# Store the results\n",
    "shrewCounts.saveAsTextFile(resultsLocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can collect up and display interesting information about the processing results.  Note that the ETL processing is not yet complete, since we can see what appear to be XML tags and partial XML tags that should be removed (or at least transformed) to produce the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words:  5203 , Total words:  15403 \n",
      "\n",
      "dir> :  364\n",
      "</stage :  182\n",
      "<stage :  181\n",
      "</petruchio> :  158\n",
      "<petruchio> :  158\n",
      "will :  146\n",
      "thou :  112\n",
      "shall :  99\n",
      "</tranio> :  91\n",
      "<tranio> :  91\n"
     ]
    }
   ],
   "source": [
    "# Count the number of unique words and the total number of words.\n",
    "# Sans stop words, of course.\n",
    "countOfUniqueWords = shrewCounts.count()\n",
    "totalCountOfWords  = shrewCounts.map(lambda t: t[1]).reduce(lambda x,y: x+y)\n",
    "\n",
    "# Look at some results.\n",
    "print('Unique words: ',countOfUniqueWords,', Total words: ',totalCountOfWords,'\\n')\n",
    "for k,v in shrewCounts.take(10):\n",
    "    print(k,': ',v)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
