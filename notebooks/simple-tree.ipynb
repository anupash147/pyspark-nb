{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Decision Tree Machine Learning Example\n",
    "\n",
    "This is a simple example of creating and training a decision tree model using the available Spark machine learning libraries. Model training and evaluation is performed, along with saving a trained model to use for future predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Spark Context to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with Spark version:  1.6.1\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "\n",
    "# Create a configuration object.\n",
    "conf = (\n",
    "    pyspark\n",
    "      .SparkConf()\n",
    "      .setMaster('local[*]')\n",
    "      .setAppName('Simple Decision Tree Notebook')\n",
    ")\n",
    "\n",
    "# Create a Spark context for local work\n",
    "try:\n",
    "    sc\n",
    "except:\n",
    "    sc = pyspark.SparkContext(conf = conf)\n",
    "    \n",
    "print('Running with Spark version: ',sc.version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Forest CoverType dataset\n",
    "\n",
    "The Covtype data set is available online from: [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/). The file _covtype.data.gz_ includes the type data and _covtype.info_ includes the metadata. This data was originally provided by Colorado State University.\n",
    "\n",
    "This dataset has also been used in a [*Kaggle*](https://www.kaggle.com/c/forest-cover-type-prediction) competition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets make sure that the file is available locally. The _urlretrieve()_ method called here is only supposed to download the file if it is not present locally. My version appears to download the file each time, so a more explicit check is performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file already present at:  data/covtype.data.gz\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "\n",
    "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/covtype/covtype.data.gz'\n",
    "localfile = 'data/covtype.data.gz'\n",
    "\n",
    "# Ensure we fetch the data if we need to.\n",
    "if(not os.path.isfile(localfile)):\n",
    "    print(\"Downloading data into: \",localfile)\n",
    "    localfile, headers = urllib.request.urlretrieve(url, localfile)\n",
    "else:\n",
    "    print(\"Data file already present at: \",localfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the data locally, we can grab it through an RDD.  Use a simple textfile RDD since this is an ASCII CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rawData = sc.textFile(localfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can transform the raw data into a sequence of LabeledPoints (from MLLib):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# Extract the dataset features and target.\n",
    "def ingest(line):\n",
    "    # Simple numeric features (some are one-hot encoded).\n",
    "    # Last field is the label (training target).\n",
    "    fields = [float(f) for f in line.split(',')]\n",
    "    features = Vectors.dense(fields[0:len(fields)-1])\n",
    "    \n",
    "    # Subtract 1 from the label to satisfy the '0' based\n",
    "    # DecisionTree model.\n",
    "    label    = fields[-1] - 1\n",
    "    return LabeledPoint(label,features)\n",
    "\n",
    "pointdata = rawData.map(ingest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our data in a format we can train a model with, lets look at a few entries to see if it is as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(4.0, [2596.0,51.0,3.0,258.0,0.0,510.0,221.0,232.0,148.0,6279.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(4.0, [2590.0,56.0,2.0,212.0,-6.0,390.0,220.0,235.0,151.0,6225.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [2804.0,139.0,9.0,268.0,65.0,3180.0,234.0,238.0,135.0,6121.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(1.0, [2785.0,155.0,18.0,242.0,118.0,3090.0,238.0,238.0,122.0,6211.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]),\n",
       " LabeledPoint(4.0, [2595.0,45.0,2.0,153.0,-1.0,391.0,220.0,234.0,150.0,6172.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pointdata.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we see that we have a sequence of LabeledPoint objects, each with a label (numeric in this case, since we ensured that the values would be converted to float values).  There is a DenseMatrix element included for each point, with 55 (float) values corresponding to the features.\n",
    "\n",
    "The first 10 features are numeric.  The next two are categorical and have been encoded as one-hot codes.  The _Wilderness Areas_ feature takes 4 columns, and the _Soil Type_ takes up 40 columns.\n",
    "\n",
    "The _Cover Type_ factor is used as the label is a numerically encoded categorical factor.  It is important to ensure that this encoded value is not treated as a number - no ordering in the feature is implied.  Each value stands alone and simply indicates a category and not a relationship with any other feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partition the Data\n",
    "\n",
    "At this point, we can partition the data into a training set, a validation set, and a testing set.  We will use the training set to train the model, and the validation set to measure how our training is performing.  We can adjust model hyperparameters to adjust training results as measured by the validation dataset without issue.\n",
    "\n",
    "We will only run predictions against the testing dataset and no information from testing will be used during training or validation.  This keeps our confidence in the test performance measurements high.\n",
    "\n",
    "For this example we can simply split the data randomly into the 3 sets of data, with 80% used for training, and 10% used for each of validation and testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "465022 57874 58116\n"
     ]
    }
   ],
   "source": [
    "trainData, cvData, testData = pointdata.randomSplit([0.8,0.1,0.1])\n",
    "trainData.cache()\n",
    "cvData.cache()\n",
    "testData.cache()\n",
    "\n",
    "print(trainData.count(),cvData.count(),testData.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that the data partitioning has separated the original dataset into suitable size segments that we can use for our different purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Decision Tree Model\n",
    "\n",
    "At this point we can go ahead and train the model.  Here we are using the Spark MLLib [DecisionTree](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.tree.DecisionTree) and [DecisionTreeModel](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.tree.DecisionTreeModel) for this purpose.  Read more about these and the hyperparameters available from the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.tree import DecisionTree, DecisionTreeModel\n",
    "\n",
    "model = DecisionTree.trainClassifier( trainData, 7, {}, \"gini\", 4, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the Validation Data\n",
    "\n",
    "At this point we have a trained model and we would like to evaluate its performance.  We do this with the validation data.  If we find that we want to increase the performance, we can adjust hyperparameters or even select a different model type depending on the outcome from this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score the validation data using the model.\n",
    "cvPredictions = model.predict( cvData.map(lambda x: x.features))\n",
    "\n",
    "# Label the results.\n",
    "cvLabelsAndPredictions = cvData.map(lambda x: x.label).zip(cvPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Validation Results\n",
    "\n",
    "Using the MLLib [MulticlassMetrics](http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.evaluation.MulticlassMetrics) object allows us to very simply evaluate the prediction performance of the model.  Here we perform the predictions, then form a labelled set of results that can be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a metrics object to evaluate the results.\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "cvMetrics = MulticlassMetrics(cvLabelsAndPredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can extract some performance results from the metrics object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision:  0.6938003248436259 \n",
      "                    recall:  0.6938003248436259 \n",
      "                  fMeasure:  0.6938003248436259 \n",
      "         weightedPrecision:  0.7403884816611251 \n",
      "            weightedRecall:  0.6938003248436259 \n",
      " weightedFalsePositiveRate:  0.18594948465009822 \n",
      "  weightedTruePositiveRate:  0.6938003248436259 \n",
      " \n",
      "Confusion Matrix:\n",
      "[[14174  5403     0     0  1132]\n",
      " [ 6606 22244   728   904    40]\n",
      " [    1   461  2898    28     0]\n",
      " [    0     3     0     8     0]\n",
      " [  343    35     0     0   829]]\n"
     ]
    }
   ],
   "source": [
    "print('                 precision: ',cvMetrics.precision(),'\\n',\n",
    "      '                   recall: ',cvMetrics.recall(),'\\n',\n",
    "      '                 fMeasure: ',cvMetrics.fMeasure(),'\\n',\n",
    "      '        weightedPrecision: ',cvMetrics.weightedPrecision,'\\n',\n",
    "      '           weightedRecall: ',cvMetrics.weightedRecall,'\\n',\n",
    "      'weightedFalsePositiveRate: ',cvMetrics.weightedFalsePositiveRate,'\\n',\n",
    "      ' weightedTruePositiveRate: ',cvMetrics.weightedTruePositiveRate,'\\n',\n",
    "      '\\nConfusion Matrix:'\n",
    "     )\n",
    "\n",
    "import numpy as np\n",
    "print(cvMetrics.confusionMatrix().toArray().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score and Evaluate the Test Data Results\n",
    "\n",
    "Once we are satisfied with the models predictive performance (hint - the performance above is not very satisfying!), we can then evaluate the model on the test data, which we expect will give slightly lower performance than the validation data since some work flows include iteration over the validation data more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Score and label the test data using the model.\n",
    "testPredictions = model.predict( testData.map(lambda x: x.features))\n",
    "testLabelsAndPredictions = testData.map(lambda x: x.label).zip(testPredictions)\n",
    "\n",
    "# Create a metrics object for the test results.\n",
    "testMetrics = MulticlassMetrics(testLabelsAndPredictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision:  0.6945935714777342 \n",
      "                    recall:  0.6945935714777342 \n",
      "                  fMeasure:  0.6945935714777342 \n",
      "         weightedPrecision:  0.7411035884452303 \n",
      "            weightedRecall:  0.694593571477734 \n",
      " weightedFalsePositiveRate:  0.18594438180801656 \n",
      "  weightedTruePositiveRate:  0.694593571477734 \n",
      " \n",
      "Confusion Matrix:\n",
      "[[14095  5353     0     0  1128]\n",
      " [ 6697 22519   733   949    35]\n",
      " [    1   465  2856    20     0]\n",
      " [    0     4     0    12     0]\n",
      " [  334    30     0     0   885]]\n"
     ]
    }
   ],
   "source": [
    "print('                 precision: ',testMetrics.precision(),'\\n',\n",
    "      '                   recall: ',testMetrics.recall(),'\\n',\n",
    "      '                 fMeasure: ',testMetrics.fMeasure(),'\\n',\n",
    "      '        weightedPrecision: ',testMetrics.weightedPrecision,'\\n',\n",
    "      '           weightedRecall: ',testMetrics.weightedRecall,'\\n',\n",
    "      'weightedFalsePositiveRate: ',testMetrics.weightedFalsePositiveRate,'\\n',\n",
    "      ' weightedTruePositiveRate: ',testMetrics.weightedTruePositiveRate,'\\n',\n",
    "      '\\nConfusion Matrix:'\n",
    "     )\n",
    "\n",
    "print(testMetrics.confusionMatrix().toArray().astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model\n",
    "\n",
    "Since we might want to use the model to perform scoring (prediction) on data at a later time, we can keep the model around.  This is especially important if we have invested much effort and time to develop and train the model to optimize its performance.\n",
    "\n",
    "This is easily done by first storing the model.  Models are stored within a specified directory which contain the data and metadata for the model.  The model data is further stored in a series of [Parquet](https://parquet.apache.org/documentation/latest/) files for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Location where the model will be stored.\n",
    "modelLocation = 'tree-model'\n",
    "\n",
    "# Ensure that there is no model currently in the location.\n",
    "# Choose to store multiple models by using multiple locations.\n",
    "import shutil\n",
    "shutil.rmtree(modelLocation,ignore_errors=True)\n",
    "\n",
    "# Actually save the model.\n",
    "model.save(sc,modelLocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Stored Model\n",
    "\n",
    "Now we can read in the stored model and use it to perform additional predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples to predict with previously stored model:  116409\n"
     ]
    }
   ],
   "source": [
    "sameModel = DecisionTreeModel.load(sc,modelLocation)\n",
    "\n",
    "otherData = pointdata.sample(False,0.2,2112)\n",
    "otherData.cache()\n",
    "print('Samples to predict with previously stored model: ',otherData.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "otherPredictions = model.predict( otherData.map(lambda x: x.features))\n",
    "otherLabelsAndPredictions = otherData.map(lambda x: x.label).zip(otherPredictions)\n",
    "otherMetrics = MulticlassMetrics(otherLabelsAndPredictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision:  0.6932625484283861 \n",
      "                    recall:  0.6932625484283861 \n",
      "                  fMeasure:  0.6932625484283861 \n",
      "         weightedPrecision:  0.7399464558364611 \n",
      "            weightedRecall:  0.6932625484283862 \n",
      " weightedFalsePositiveRate:  0.1860846008923653 \n",
      "  weightedTruePositiveRate:  0.6932625484283862 \n",
      " \n",
      "Confusion Matrix:\n",
      "[[28393 10769     0     0  2270]\n",
      " [13434 44836  1474  1848    69]\n",
      " [    2   930  5683    57     0]\n",
      " [    0    12     0    25     0]\n",
      " [  669    75     0     0  1765]]\n"
     ]
    }
   ],
   "source": [
    "print('                 precision: ',otherMetrics.precision(),'\\n',\n",
    "      '                   recall: ',otherMetrics.recall(),'\\n',\n",
    "      '                 fMeasure: ',otherMetrics.fMeasure(),'\\n',\n",
    "      '        weightedPrecision: ',otherMetrics.weightedPrecision,'\\n',\n",
    "      '           weightedRecall: ',otherMetrics.weightedRecall,'\\n',\n",
    "      'weightedFalsePositiveRate: ',otherMetrics.weightedFalsePositiveRate,'\\n',\n",
    "      ' weightedTruePositiveRate: ',otherMetrics.weightedTruePositiveRate,'\\n',\n",
    "      '\\nConfusion Matrix:'\n",
    "     )\n",
    "\n",
    "print(otherMetrics.confusionMatrix().toArray().astype(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
